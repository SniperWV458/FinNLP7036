# -*- coding: utf-8 -*-
"""all in one NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DLQIooP7kNYHztQ7tHu5eO9NPNDPxIrY
"""

import os
import time
import shutil
import requests
import pandas as pd
from datetime import datetime, timedelta
from urllib.parse import urlencode, urlparse
from bs4 import BeautifulSoup
from transformers import pipeline

# ----------------- Functions -----------------

def generate_google_news_link(query, start_date, end_date):
    """
    Generate a Google News search URL with custom date range.
    """
    base_url = "https://www.google.com/search"
    params = {
        "q": query,
        "tbs": f"cdr:1,cd_min:{start_date},cd_max:{end_date}",
        "tbm": "nws"
    }
    return f"{base_url}?{urlencode(params)}"

def get_news_links(search_url):
    """
    Fetch news article links from a Google News search results page.
    """
    headers = {
        "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/91.0.4472.124 Safari/537.36")
    }
    try:
        response = requests.get(search_url, headers=headers)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        links = []
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if "https://" in href and "google.com" not in href:
                links.append(href)
        return set(links)  # Remove duplicates
    except requests.exceptions.RequestException as e:
        print(f"Failed to retrieve {search_url}: {e}")
        return []

def remove_empty_lines(file_path):
    """
    Remove empty lines from a file.
    """
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    non_empty_lines = [line for line in lines if line.strip() != ""]
    with open(file_path, "w", encoding="utf-8") as f:
        f.writelines(non_empty_lines)
    print(f"Removed empty lines from: {file_path}")

def extract_article_details(url):
    """
    Extract article details: title and first 500 words.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.title.string if soup.title else "No title found"
        paragraphs = soup.find_all('p')
        article_text = " ".join([p.get_text() for p in paragraphs])
        first_500_words = " ".join(article_text.split()[:500])
        return title, first_500_words
    except requests.exceptions.RequestException as e:
        print(f"Failed to retrieve {url}: {e}")
        return "Error", "Error"

def add_sentiment_labels(csv_path, sentiment_pipeline):
    """
    Run sentiment analysis on each article's first 500 words and update CSV.
    """
    df = pd.read_csv(csv_path)
    if 'First_500_Words' not in df.columns:
        print(f"Skipping {csv_path} (no 'First_500_Words' column found)")
        return
    sentiment_labels = []
    sentiment_scores = []
    for text in df['First_500_Words']:
        if isinstance(text, str):
            truncated_text = text[:500]  # Truncate to first 500 characters
            sentiment = sentiment_pipeline(truncated_text)[0]
            sentiment_labels.append(sentiment['label'])
            sentiment_scores.append(sentiment['score'])
        else:
            sentiment_labels.append("Error")
            sentiment_scores.append(0.0)
    df['Sentiment_Label'] = sentiment_labels
    df['Sentiment_Score'] = sentiment_scores
    df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"Updated sentiment for {csv_path}")

def calculate_average_score(csv_path):
    """
    Calculate the average sentiment score for a query group CSV.
    """
    df = pd.read_csv(csv_path)
    if 'Sentiment_Label' not in df.columns or 'Sentiment_Score' not in df.columns:
        print(f"Skipping {csv_path} (required columns missing)")
        return None
    positive_scores = df[df['Sentiment_Label'] == 'positive']['Sentiment_Score']
    negative_scores = df[df['Sentiment_Label'] == 'negative']['Sentiment_Score']
    sum_positive = positive_scores.sum()
    sum_negative = negative_scores.sum()
    num_positive = len(positive_scores)
    num_negative = len(negative_scores)
    if num_positive + num_negative == 0:
        return None
    average_score = (sum_positive - sum_negative) / (num_positive + num_negative)
    return average_score

def process_monthly_summaries(sentiment_base_folder, final_results_folder):
    """
    Process monthly summaries and create a unified matrix.
    """
    all_months_data = []
    for year_folder in os.listdir(sentiment_base_folder):
        year_path = os.path.join(sentiment_base_folder, year_folder)
        if os.path.isdir(year_path):
            target_year_path = os.path.join(final_results_folder, year_folder)
            os.makedirs(target_year_path, exist_ok=True)
            for month_folder in os.listdir(year_path):
                month_path = os.path.join(year_path, month_folder)
                if os.path.isdir(month_path):
                    summary_csv_path = os.path.join(month_path, "monthly_summary.csv")
                    if os.path.exists(summary_csv_path):
                        target_month_path = os.path.join(target_year_path, month_folder)
                        os.makedirs(target_month_path, exist_ok=True)
                        shutil.copy(summary_csv_path, target_month_path)
                        month_data = pd.read_csv(summary_csv_path)
                        month_data['Month'] = month_folder
                        month_data['Year'] = year_folder
                        all_months_data.append(month_data)
    if all_months_data:
        unified_df = pd.concat(all_months_data)
        unified_matrix = unified_df.pivot_table(index=['Year', 'Month'], columns='Query Group', values='Average Score')
        unified_matrix_path = os.path.join(final_results_folder, "unified_matrix.csv")
        unified_matrix.to_csv(unified_matrix_path, encoding="utf-8")
        print(f"Unified matrix has been created at: {unified_matrix_path}")
    else:
        print("No data available to create the unified matrix.")

# ----------------- Main Code -----------------

# 1. Generate Google News search links and save to CSV
search_queries = [
    ["SP 500", "S&P 500", "Standard and Poor's 500", "^GSPC", "S&P 500 Index", "SPX", "S&P 500 ETF", "S&P500", "S&P Index", "Standard & Poor's 500 Index", "S&P 500 Stock Market", "US Stock Market", "American Stocks", "USA Economy", "U.S. Markets", "U.S. Economy", "Wall Street Index", "US Equity Market", "U.S. Stock Exchange", "S&P 500 Companies"],
    ["NASDAQ", "NASDAQ Composite", "NASDAQ Index", "^IXIC", "Nasdaq Composite Index", "NASDAQ-100", "Nasdaq 100", "NASDAQ-100 Index", "NASDAQ stocks", "NASDAQ Index ETF", "American Technology Stocks", "U.S. Tech Stocks", "Tech-heavy Index", "USA Stock Market", "Nasdaq Tech Index", "NASDAQ Growth", "Silicon Valley Stocks", "NASDAQ 100 Tech", "USA Technology", "Tech Stocks Index"],
    ["Dow Jones", "DJIA", "Dow Jones Industrial Average", "^DJI", "Dow Jones Index", "Dow Jones Average", "DJIA Index", "Dow Jones Industrial", "Dow Jones Industrial Stocks", "DJIA ETF", "US Blue-Chip Stocks", "USA Industrial Stocks", "U.S. Market Leaders", "Wall Street Benchmark", "Dow Jones Companies", "US Industrials", "American Economy", "US Industrial Market", "US Stock Index", "Wall Street Giants"],
    ["CAC 40", "Paris Stock Exchange", "Euronext Paris", "^FCHI", "French Stock Market", "Paris Index", "French Economy", "France Stock Exchange", "CAC 40 Companies", "Paris Bourse", "French Markets", "Paris Exchange", "Euronext Index", "French Blue Chip Stocks", "French Equity Market", "France Stock Index", "Paris Market", "Eurozone Stocks", "France Economy", "Eurozone Market"],
    ["FTSE 100", "London Stock Exchange", "UK 100 Index", "^FTSE", "FTSE 100 Index", "London Index", "UK Stock Market", "British Stock Exchange", "UK Economy", "FTSE 100 Companies", "London Market", "UK Economy Stocks", "FTSE Index ETF", "British Blue Chips", "London Exchange", "UK Markets", "British Economy", "UK Financial Markets", "London Stock Index", "UK Stock Exchange", "FTSE 100 Stocks"],
    ["^STOXX50E", "EuroStoxx 50", "EuroStoxx 50 Index", "European Stock Market", "Eurozone Stocks", "European Economy", "Stoxx Europe 50", "Eurozone 50 Index", "EuroStoxx Index", "Europe Market Leaders", "Eurozone Leaders", "Top European Stocks", "Eurozone Top Companies", "European Blue Chips", "European Equity Market", "Eurozone Financials", "Eurozone Benchmark", "European Blue Chip Stocks", "Eurozone Economic Index", "European Market Index"],
    ["^N225", "Nikkei 225", "Nikkei Index", "Japanese Stock Market", "Japan Economy", "Nikkei Average", "Tokyo Stock Exchange", "Japan Top 225 Stocks", "Japan Stock Index", "Japanese Equity Market", "Nikkei 225 Companies", "Japan's Leading Stocks", "Japanese Financial Market", "Tokyo Exchange", "Nikkei Market", "Japan Economic Index", "Japanese Economy", "Japan's Stock Exchange", "Top Japanese Companies", "Nikkei 225 ETF"],
    ["^HSI", "Hang Seng", "Hang Seng Index", "Hong Kong Stock Market", "Hong Kong Economy", "Hong Kong Exchange", "HSI Index", "Hang Seng Index ETF", "Hong Kong Financial Market", "Chinese Stock Market", "HSI Stocks", "Asia-Pacific Stocks", "Hong Kong Blue Chips", "Hong Kong's Leading Stocks", "HSI Index Stocks", "Asian Financial Market", "Hong Kong Leading Companies", "Asian Market Leaders", "Hang Seng Companies", "Hong Kong Stock Index"],
    ["000001.SS", "Shanghai Composite", "Shanghai Stock Exchange", "Chinese Stock Market", "China Economy", "Shanghai Index", "China Financial Market", "Shanghai Composite Index", "Shanghai Exchange", "Chinese Stock Index", "China's Leading Stocks", "Shanghai Exchange Companies", "China Blue Chip Stocks", "Chinese Equity Market", "Shanghai Financial Index", "China Benchmark", "Shanghai Composite ETF", "China's Leading Companies", "Chinese Market Leaders", "China's Economic Stocks"],
    ["^BSESN", "Bombay Sensex", "S&P BSE Sensex", "Indian Stock Market", "India Economy", "BSE Sensex", "Mumbai Stock Exchange", "Sensex Companies", "Indian Financial Market", "Sensex 30", "BSE 30 Index", "India's Leading Stocks", "Indian Market Leaders", "Indian Equity Market", "Bombay Exchange", "Indian Stock Index", "Sensex Index ETF", "BSE India", "Indian Economy Stocks", "Bombay Financial Index"],
    ["^NSEI", "Nifty 50", "National Stock Exchange of India", "Indian Stock Index", "India Stock Market", "Nifty Index", "India Economy", "Indian Market Leaders", "Nifty 50 Stocks", "NSE India", "Indian Blue Chips", "Indian Financial Market", "Indian Stock Market", "India 50 Index", "Nifty Index ETF", "India's Top 50", "India's Leading Stocks", "Nifty 50 Companies", "India Economic Stocks", "Indian Market Index"],
    ["^KS11", "KOSPI", "Korea Composite Stock Price Index", "Korean Stock Market", "Korea Economy", "KOSPI Index", "South Korean Stock Market", "Korean Market Leaders", "KOSPI 200", "South Korea Financial Market", "Korean Exchange", "KOSPI ETF", "Korean Leading Stocks", "South Korean Economy", "Korean Market Index", "South Korea Stock Index", "Korean Economy Stocks", "South Korea Exchange", "Korean Equity Market", "Korea Stock Index"],
    ["Gold", "XAU", "Gold Price", "Gold Market", "Precious Metals", "Gold Spot", "Gold Bullion", "Gold ETF", "Gold Investment", "Gold Mining", "Gold Futures", "Gold Stocks", "Gold Index", "Gold Commodity", "Gold Trading", "Gold Bullion ETF", "Gold Commodity Index", "Gold Market Trends", "Gold Investment Funds", "Gold Prices Today"],
    ["Silver", "XAG", "Silver Price", "Silver Market", "Precious Metals", "Silver Spot", "Silver Bullion", "Silver ETF", "Silver Investment", "Silver Mining", "Silver Futures", "Silver Stocks", "Silver Index", "Silver Commodity", "Silver Trading", "Silver Bullion ETF", "Silver Commodity Index", "Silver Market Trends", "Silver Investment Funds", "Silver Prices Today"],
    ["Oil", "Crude Oil", "WTI", "Brent Crude", "Oil Price", "Crude Oil Price", "OPEC", "Oil Futures", "Oil Market", "Oil Stocks", "Oil ETF", "Global Oil Supply", "Oil Trading", "Oil Production", "Brent Oil Futures", "Oil Price Index", "Oil Investment", "Oil Exploration", "Oil Trading Market", "Oil Price Trends"]
]

start_year = 2003
end_year = 2024
links = []

# Loop through each query group, year, and month to generate search links
for query_group in search_queries:
    for query in query_group:
        for year in range(start_year, end_year + 1):
            for month in range(1, 13):
                start_date = f"{month}/1/{year}"
                next_month = month % 12 + 1
                next_year = year if month < 12 else year + 1
                end_date = (datetime(next_year, next_month, 1) - timedelta(days=1)).strftime("%m/%d/%Y")
                link = generate_google_news_link(query, start_date, end_date)
                links.append([query, start_date, end_date, link])

# Save generated links to CSV
df_links = pd.DataFrame(links, columns=["Query", "Start Date", "End Date", "Google News Link"])
csv_filename = "links.csv"
df_links.to_csv(csv_filename, index=False)

# 2. Fetch news links and save results in text files under "google_news_results"
output_base_folder = "google_news_results"
os.makedirs(output_base_folder, exist_ok=True)
df_links = pd.read_csv("links.csv")

for index, row in df_links.iterrows():
    query = row["Query"].replace(" ", "_")
    search_url = row["Google News Link"]
    query_folder = os.path.join(output_base_folder, query)
    os.makedirs(query_folder, exist_ok=True)
    news_links = get_news_links(search_url)
    start_date_sanitized = row['Start Date'].replace("/", "_")
    end_date_sanitized = row['End Date'].replace("/", "_")
    file_path = os.path.join(query_folder, f"{start_date_sanitized}_to_{end_date_sanitized}.txt")
    with open(file_path, "w", encoding="utf-8") as f:
        for link in news_links:
            f.write(link + "\n")
    print(f"Saved {len(news_links)} links for {query} ({row['Start Date']} to {row['End Date']})")
    time.sleep(3)

# Cleanup: Move non-empty files to year/month structure and delete empty files
for query in os.listdir(output_base_folder):
    query_path = os.path.join(output_base_folder, query)
    if os.path.isdir(query_path):
        for file_name in os.listdir(query_path):
            file_path = os.path.join(query_path, file_name)
            if os.path.isfile(file_path):
                if os.path.getsize(file_path) == 0:
                    os.remove(file_path)
                    print(f"Deleted empty file: {file_path}")
                else:
                    try:
                        date_part = file_name.split("_to_")[0]
                        date_obj = datetime.strptime(date_part, "%m_%d_%Y")
                        year = date_obj.year
                        month_name = date_obj.strftime("%B")
                        new_location = os.path.join(output_base_folder, str(year), month_name, query)
                        os.makedirs(new_location, exist_ok=True)
                        os.rename(file_path, os.path.join(new_location, file_name))
                        print(f"Moved file to {new_location}")
                    except ValueError:
                        print(f"Skipping file due to incorrect date format: {file_name}")

# Delete any now-empty group folders
for query in os.listdir(output_base_folder):
    query_path = os.path.join(output_base_folder, query)
    if os.path.isdir(query_path) and not os.listdir(query_path):
        os.rmdir(query_path)
        print(f"Deleted empty group folder: {query_path}")
print("Cleanup and organization complete!")

# 3. Create unified text files for each query group
# Note: Renaming the search_quieries dictionary to query_groups for consistency
query_groups = {
    "SP_500": ["SP_500", "S&P_500", "Standard_and_Poor's_500", "^GSPC", "S&P_500_Index", "SPX", "S&P_500_ETF", "S&P500", "S&P_Index", "Standard_&_Poor's_500_Index", "S&P_500_Stock_Market", "US_Stock_Market", "American_Stocks", "USA_Economy", "U.S._Markets", "U.S._Economy", "Wall_Street_Index", "US_Equity_Market", "U.S._Stock_Exchange", "S&P_500_Companies"],
    "NASDAQ": ["NASDAQ", "NASDAQ_Composite", "NASDAQ_Index", "^IXIC", "Nasdaq_Composite_Index", "NASDAQ-100", "Nasdaq_100", "NASDAQ-100_Index", "NASDAQ_stocks", "NASDAQ_Index_ETF", "American_Technology_Stocks", "U.S._Tech_Stocks", "Tech-heavy_Index", "USA_Stock_Market", "Nasdaq_Tech_Index", "NASDAQ_Growth", "Silicon_Valley_Stocks", "NASDAQ_100_Tech", "USA_Technology", "Tech_Stocks_Index"],
    "Dow_Jones": ["Dow_Jones", "DJIA", "Dow_Jones_Industrial_Average", "^DJI", "Dow_Jones_Index", "Dow_Jones_Average", "DJIA_Index", "Dow_Jones_Industrial", "Dow_Jones_Industrial_Stocks", "DJIA_ETF", "US_Blue-Chip_Stocks", "USA_Industrial_Stocks", "U.S._Market_Leaders", "Wall_Street_Benchmark", "Dow_Jones_Companies", "US_Industrials", "American_Economy", "US_Industrial_Market", "US_Stock_Index", "Wall_Street_Giants"],
    "CAC_40": ["CAC_40", "Paris_Stock_Exchange", "Euronext_Paris", "^FCHI", "French_Stock_Market", "Paris_Index", "French_Economy", "France_Stock_Exchange", "CAC_40_Companies", "Paris_Bourse", "French_Markets", "Paris_Exchange", "Euronext_Index", "French_Blue_Chip_Stocks", "French_Equity_Market", "France_Stock_Index", "Paris_Market", "Eurozone_Stocks", "France_Economy", "Eurozone_Market"],
    "FTSE_100": ["FTSE_100", "London_Stock_Exchange", "UK_100_Index", "^FTSE", "FTSE_100_Index", "London_Index", "UK_Stock_Market", "British_Stock_Exchange", "UK_Economy", "FTSE_100_Companies", "London_Market", "UK_Economy_Stocks", "FTSE_Index_ETF", "British_Blue_Chips", "London_Exchange", "UK_Markets", "British_Economy", "UK_Financial_Markets", "London_Stock_Index", "UK_Stock_Exchange", "FTSE_100_Stocks"],
    "EuroStoxx_50": ["^STOXX50E", "EuroStoxx_50", "EuroStoxx_50_Index", "European_Stock_Market", "Eurozone_Stocks", "European_Economy", "Stoxx_Europe_50", "Eurozone_50_Index", "EuroStoxx_Index", "Europe_Market_Leaders", "Eurozone_Leaders", "Top_European_Stocks", "Eurozone_Top_Companies", "European_Blue_Chips", "European_Equity_Market", "Eurozone_Financials", "Eurozone_Benchmark", "European_Blue_Chip_Stocks", "Eurozone_Economic_Index", "European_Market_Index"],
    "Nikkei_225": ["^N225", "Nikkei_225", "Nikkei_Index", "Japanese_Stock_Market", "Japan_Economy", "Nikkei_Average", "Tokyo_Stock_Exchange", "Japan_Top_225_Stocks", "Japan_Stock_Index", "Japanese_Equity_Market", "Nikkei_225_Companies", "Japan's_Leading_Stocks", "Japanese_Financial_Market", "Tokyo_Exchange", "Nikkei_Market", "Japan_Economic_Index", "Japanese_Economy", "Japan's_Stock_Exchange", "Top_Japanese_Companies", "Nikkei_225_ETF"],
    "Hang_Seng": ["^HSI", "Hang_Seng", "Hang_Seng_Index", "Hong_Kong_Stock_Market", "Hong_Kong_Economy", "Hong_Kong_Exchange", "HSI_Index", "Hang_Seng_Index_ETF", "Hong_Kong_Financial_Market", "Chinese_Stock_Market", "HSI_Stocks", "Asia-Pacific_Stocks", "Hong_Kong_Blue_Chips", "Hong_Kong's_Leading_Stocks", "HSI_Index_Stocks", "Asian_Financial_Market", "Hong_Kong_Leading_Companies", "Asian_Market_Leaders", "Hang_Seng_Companies", "Hong_Kong_Stock_Index"],
    "Shanghai_Composite": ["000001.SS", "Shanghai_Composite", "Shanghai_Stock_Exchange", "Chinese_Stock_Market", "China_Economy", "Shanghai_Index", "China_Financial_Market", "Shanghai_Composite_Index", "Shanghai_Exchange", "Chinese_Stock_Index", "China's_Leading_Stocks", "Shanghai_Exchange_Companies", "China_Blue_Chip_Stocks", "Chinese_Equity_Market", "Shanghai_Financial_Index", "China_Benchmark", "Shanghai_Composite_ETF", "China's_Leading_Companies", "Chinese_Market_Leaders", "China's_Economic_Stocks"],
    "Bombay_Sensex": ["^BSESN", "Bombay_Sensex", "S&P_BSE_Sensex", "Indian_Stock_Market", "India_Economy", "BSE_Sensex", "Mumbai_Stock_Exchange", "Sensex_Companies", "Indian_Financial_Market", "Sensex_30", "BSE_30_Index", "India's_Leading_Stocks", "Indian_Market_Leaders", "Indian_Equity_Market", "Bombay_Exchange", "Indian_Stock_Index", "Sensex_Index_ETF", "BSE_India", "Indian_Economy_Stocks", "Bombay_Financial_Index"],
    "Nifty_50": ["^NSEI", "Nifty_50", "National_Stock_Exchange_of_India", "Indian_Stock_Index", "India_Stock_Market", "Nifty_Index", "India_Economy", "Indian_Market_Leaders", "Nifty_50_Stocks", "NSE_India", "Indian_Blue_Chips", "Indian_Financial_Market", "Indian_Stock_Market", "India_50_Index", "Nifty_Index_ETF", "India's_Top_50", "India's_Leading_Stocks", "Nifty_50_Companies", "India_Economic_Stocks", "Indian_Market_Index"],
    "KOSPI": ["^KS11", "KOSPI", "Korea_Composite_Stock_Price_Index", "Korean_Stock_Market", "Korea_Economy", "KOSPI_Index", "South_Korean_Stock_Market", "Korean_Market_Leaders", "KOSPI_200", "South_Korea_Financial_Market", "Korean_Exchange", "KOSPI_ETF", "Korean_Leading_Stocks", "South_Korean_Economy", "Korean_Market_Index", "South_Korea_Stock_Index", "Korean_Economy_Stocks", "South_Korea_Exchange", "Korean_Equity_Market", "Korea_Stock_Index"],
    "Gold": ["Gold", "XAU", "Gold_Price", "Gold_Market", "Precious_Metals", "Gold_Spot", "Gold_Bullion", "Gold_ETF", "Gold_Investment", "Gold_Mining", "Gold_Futures", "Gold_Stocks", "Gold_Index", "Gold_Commodity", "Gold_Trading", "Gold_Bullion_ETF", "Gold_Commodity_Index", "Gold_Market_Trends", "Gold_Investment_Funds", "Gold_Prices_Today"],
    "Silver": ["Silver", "XAG", "Silver_Price", "Silver_Market", "Precious_Metals", "Silver_Spot", "Silver_Bullion", "Silver_ETF", "Silver_Investment", "Silver_Mining", "Silver_Futures", "Silver_Stocks", "Silver_Index", "Silver_Commodity", "Silver_Trading", "Silver_Bullion_ETF", "Silver_Commodity_Index", "Silver_Market_Trends", "Silver_Investment_Funds", "Silver_Prices_Today"],
    "Oil": ["Oil", "Crude_Oil", "WTI", "Brent_Crude", "Oil_Price", "Crude_Oil_Price", "OPEC", "Oil_Futures", "Oil_Market", "Oil_Stocks", "Oil_ETF", "Global_Oil_Supply", "Oil_Trading", "Oil_Production", "Brent_Oil_Futures", "Oil_Price_Index", "Oil_Investment", "Oil_Exploration", "Oil_Trading_Market", "Oil_Price_Trends"]
}

# For each year/month folder in output_base_folder, create a unified text file per query group
for year_folder in os.listdir(output_base_folder):
    year_path = os.path.join(output_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                for query_group, query_list in query_groups.items():
                    unified_file_path = os.path.join(month_path, f"{query_group}_united.txt")
                    with open(unified_file_path, "w", encoding="utf-8") as unified_file:
                        for query in query_list:
                            query_folder_path = os.path.join(month_path, query)
                            if os.path.isdir(query_folder_path):
                                for file_name in os.listdir(query_folder_path):
                                    file_path = os.path.join(query_folder_path, file_name)
                                    if file_path.endswith(".txt") and os.path.isfile(file_path):
                                        with open(file_path, "r", encoding="utf-8") as f:
                                            content = f.read()
                                            unified_file.write(content + "\n")
                                        os.remove(file_path)
                                        print(f"Deleted {file_path}")
                                os.rmdir(query_folder_path)
                                print(f"Deleted empty folder: {query_folder_path}")
                    print(f"Created unified file: {unified_file_path}")

# Delete empty year/month folders
for year_folder in os.listdir(output_base_folder):
    year_path = os.path.join(output_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path) and not os.listdir(month_path):
                os.rmdir(month_path)
                print(f"Deleted empty month folder: {month_path}")
        if not os.listdir(year_path):
            os.rmdir(year_path)
            print(f"Deleted empty year folder: {year_path}")
print("Cleanup and organization complete!")

# 4. Remove empty lines from text files within query folders
for year_folder in os.listdir(output_base_folder):
    year_path = os.path.join(output_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                for query_folder in os.listdir(month_path):
                    query_folder_path = os.path.join(month_path, query_folder)
                    if os.path.isdir(query_folder_path):
                        for file_name in os.listdir(query_folder_path):
                            file_path = os.path.join(query_folder_path, file_name)
                            if file_path.endswith(".txt") and os.path.isfile(file_path):
                                remove_empty_lines(file_path)
print("Empty lines removal completed.")

# 5. Extract article details and create CSV files
for year_folder in os.listdir(output_base_folder):
    year_path = os.path.join(output_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                for file_name in os.listdir(month_path):
                    if file_name.endswith("_united.txt"):
                        file_path = os.path.join(month_path, file_name)
                        with open(file_path, "r", encoding="utf-8") as f:
                            urls = f.readlines()
                        article_data = []
                        for url in urls:
                            url = url.strip()
                            title, first_500_words = extract_article_details(url)
                            article_data.append([url, title, first_500_words])
                        csv_file_path = os.path.splitext(file_path)[0] + ".csv"
                        df_articles = pd.DataFrame(article_data, columns=["URL", "Title", "First_500_Words"])
                        df_articles.to_csv(csv_file_path, index=False, encoding="utf-8")
                        print(f"Created CSV for {file_name}: {csv_file_path}")
print("Data extraction and CSV creation completed.")

# 6. Move CSV files to the "sentiment_analysis" folder
sentiment_base_folder = "sentiment_analysis"
os.makedirs(sentiment_base_folder, exist_ok=True)
for year_folder in os.listdir(output_base_folder):
    year_path = os.path.join(output_base_folder, year_folder)
    if os.path.isdir(year_path):
        target_year_path = os.path.join(sentiment_base_folder, year_folder)
        os.makedirs(target_year_path, exist_ok=True)
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                target_month_path = os.path.join(target_year_path, month_folder)
                os.makedirs(target_month_path, exist_ok=True)
                for file_name in os.listdir(month_path):
                    if file_name.endswith(".csv"):
                        file_path = os.path.join(month_path, file_name)
                        target_file_path = os.path.join(target_month_path, file_name)
                        shutil.move(file_path, target_file_path)
                        print(f"Moved {file_name} to {target_file_path}")
print("CSV files have been moved to the sentiment_analysis folder.")

# 7. Run sentiment analysis on CSV files using FinBERT
sentiment_pipeline = pipeline("text-classification", model="ProsusAI/finbert")
for year_folder in os.listdir(sentiment_base_folder):
    year_path = os.path.join(sentiment_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                for file_name in os.listdir(month_path):
                    if file_name.endswith(".csv"):
                        csv_path = os.path.join(month_path, file_name)
                        add_sentiment_labels(csv_path, sentiment_pipeline)
print("Sentiment analysis labels and scores have been added.")

# 8. Create monthly summary CSV files from sentiment results
for year_folder in os.listdir(sentiment_base_folder):
    year_path = os.path.join(sentiment_base_folder, year_folder)
    if os.path.isdir(year_path):
        for month_folder in os.listdir(year_path):
            month_path = os.path.join(year_path, month_folder)
            if os.path.isdir(month_path):
                summary_data = []
                print(f"Processing {month_folder}...")
                for file_name in os.listdir(month_path):
                    if file_name.endswith(".csv"):
                        csv_path = os.path.join(month_path, file_name)
                        query_group_name = file_name.replace("_united.csv", "")
                        avg_score = calculate_average_score(csv_path)
                        if avg_score is not None:
                            print(f"Adding {query_group_name} with score: {avg_score}")
                            summary_data.append([query_group_name, avg_score])
                if summary_data:
                    summary_df = pd.DataFrame(summary_data, columns=["Query Group", "Average Score"])
                    summary_csv_path = os.path.join(month_path, "monthly_summary.csv")
                    summary_df.to_csv(summary_csv_path, index=False, encoding="utf-8")
                    print(f"Created monthly summary for {month_folder}: {summary_csv_path}")
                else:
                    print(f"No data for {month_folder}, skipping.")
print("Monthly summaries have been created.")

# 9. Process monthly summaries to create a unified matrix
final_results_folder = "final_results"
os.makedirs(final_results_folder, exist_ok=True)
process_monthly_summaries(sentiment_base_folder, final_results_folder)
print("Final results have been organized and the unified matrix has been created.")

"""fake matrix for training"""



